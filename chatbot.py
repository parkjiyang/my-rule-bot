import streamlit as st
import json
import os
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# --- 1. ì„¤ì • ë° UI ìµœì í™” ---
st.set_page_config(layout="centered", page_title="ì—…ë¬´ê·œì • AI", page_icon="ğŸ”—")
st.markdown("""
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <style>
        .main .block-container {
            padding-top: 2rem; padding-left: 1rem; padding-right: 1rem;
        }
        .stButton>button {
            height: 3.5em; font-size: 1.1em; font-weight: bold; border-radius: 10px;
        }
        .stChatMessage {
            border-radius: 10px; padding: 1em 1.2em; margin-bottom: 1em;
            border: 1px solid #e0e0e0; box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        }
    </style>
""", unsafe_allow_html=True)

# (ì´ì „ê³¼ ë™ì¼í•œ CATEGORIES ë”•ì…”ë„ˆë¦¬ - ì „ì²´ ëª©ë¡ì´ ì±„ì›Œì§„ ìƒíƒœ)
CATEGORIES = {
    "1": {
        "name": "ê¸°ë³¸ë²•ê·œ",
        "search_terms": ["í•œêµ­êµí†µì•ˆì „ê³µë‹¨ ì •ê´€"],
        "pdf_files": ["í•œêµ­êµí†µì•ˆì „ê³µë‹¨ ì •ê´€(2023ë…„ 3ì›” ê°œì •).pdf"]
    },
    "2": {
        "name": "ì¡°ì§",
        "search_terms": ["êµ­ì™¸ì‚¬ë¬´ì†Œ ìš´ì˜ì„¸ì¹™", "ìœ„ì„ì „ê²°ê·œì •", "ì´ì‚¬íšŒê·œì •", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ìš´ì˜ê·œì •", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ìœ„ì„ì „ê²°ì„¸ì¹™", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì§ì œì‹œí–‰ì„¸ì¹™", "ì§ì œê·œì •", "ì§ì œì‹œí–‰ì„¸ì¹™"],
        "pdf_files": [
            "êµ­ì™¸ì‚¬ë¬´ì†Œ ìš´ì˜ì„¸ì¹™(2023ë…„ 6ì›” ê°œì •).pdf", "ìœ„ì„ì „ê²°ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf", "ì´ì‚¬íšŒê·œì •(2023ë…„ 6ì›” ê°œì •).pdf",
            "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ìš´ì˜ê·œì •(ì „ë¬¸ 2019ë…„ 4ì›” 16ì¼).pdf", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ìœ„ì„ì „ê²°ì„¸ì¹™(2024ë…„ 2ì›” ê°œì •).pdf",
            "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì§ì œì‹œí–‰ì„¸ì¹™(2024ë…„ 12ì›” ê°œì •).pdf", "ì§ì œê·œì •(2024ë…„ 11ì›” ê°œì •).pdf", "ì§ì œì‹œí–‰ì„¸ì¹™(2024ë…„ 12ì›” ê°œì •).pdf"
        ]
    },
    "3": {
        "name": "ì¸ì‚¬",
        "search_terms": ["ë³´ì§ê´€ë¦¬ì—ê´€í•œì§€ì¹¨", "ì•ˆì „ì‚¬ê³ ì— ê´€í•œ ì„ì› ë¬¸ì±…ê·œì •", "ì¸ì‚¬ê´€ë¦¬ì„¸ì¹™", "ì¸ì‚¬ê·œì •", "ì„ê¸ˆí”¼í¬ì œ ìš´ì˜ê·œì •", "ì„ì›ì¶”ì²œìœ„ì›íšŒ ìš´ì˜ê·œì •", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì¸ì‚¬ì„¸ì¹™", "ì±„ìš©ì—…ë¬´ì²˜ë¦¬ì„¸ì¹™", "ì·¨ì—…ê·œì •"],
        "pdf_files": [
            "ë³´ì§ê´€ë¦¬ì—ê´€í•œì§€ì¹¨ 171207.pdf", "ì•ˆì „ì‚¬ê³ ì— ê´€í•œ ì„ì› ë¬¸ì±…ê·œì •(20200827).pdf", "ì¸ì‚¬ê´€ë¦¬ì„¸ì¹™(2024ë…„ 6ì›” ê°œì •).pdf",
            "ì¸ì‚¬ê·œì •(2025ë…„ 1ì›” ê°œì •).pdf", "ì„ê¸ˆí”¼í¬ì œ ìš´ì˜ê·œì •(ì „ë¬¸ 2019ë…„ 2ì›” 26ì¼ ê°œì •).pdf", "ì„ì›ì¶”ì²œìœ„ì›íšŒ ìš´ì˜ê·œì •(2022ë…„ 7ì›” ê°œì •).pdf",
            "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì¸ì‚¬ì„¸ì¹™(2023ë…„ 6ì›” ê°œì •).pdf", "ì±„ìš©ì—…ë¬´ì²˜ë¦¬ì„¸ì¹™(2024ë…„ 5ì›” ê°œì •).pdf", "ì·¨ì—…ê·œì •(2025ë…„ 1ì›” ê°œì •).pdf"
        ]
    },
    "4": {
        "name": "ë³´ìˆ˜,ë³µë¦¬",
        "search_terms": ["ê°•ì‚¬ë£Œë“±ì œìˆ˜ë‹¹ì§€ê¸‰ì„¸ì¹™", "ê²½ì˜ì„±ê³¼ê¸‰ì§€ê¸‰ì§€ì¹¨", "ë³´ìˆ˜ê·œì •", "ë³µë¦¬í›„ìƒê·œì •", "ì‹¤ë¬´ì§ê·¼ë¡œì ë° ê¸°ê°„ì œê·¼ë¡œì ê´€ë¦¬ê·œì •"],
        "pdf_files": [
            "ê°•ì‚¬ë£Œë“±ì œìˆ˜ë‹¹ì§€ê¸‰ì„¸ì¹™(2022ë…„ 11ì›” ê°œì •).pdf", "ê²½ì˜ì„±ê³¼ê¸‰ì§€ê¸‰ì§€ì¹¨(2024ë…„ 12ì›” ê°œì •).pdf", "ë³´ìˆ˜ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf",
            "ë³µë¦¬í›„ìƒê·œì •(2024ë…„ 3ì›” ê°œì •).pdf", "ì‹¤ë¬´ì§ê·¼ë¡œì ë° ê¸°ê°„ì œê·¼ë¡œì ê´€ë¦¬ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf"
        ]
    },
    "5": {
        "name": "ì¬ë¬´,ê³„ì•½",
        "search_terms": ["ê³„ì•½ì‚¬ë¬´ì²˜ë¦¬ì„¸ì¹™", "ì˜ˆì‚°íšŒê³„ê·œì •", "ìê¸ˆìš´ìš©ì—…ë¬´ì— ê´€í•œ ì„¸ì¹™", "íˆ¬ì ë° ìê¸ˆìš´ìš©ì—…ë¬´ ì‹¬ì‚¬ì— ê´€í•œ ì„¸ì¹™"],
        "pdf_files": [
            "ê³„ì•½ì‚¬ë¬´ì²˜ë¦¬ì„¸ì¹™(2025ë…„ 3ì›” ê°œì •).pdf", "ì˜ˆì‚°íšŒê³„ê·œì •(2025ë…„ 5ì›” ê°œì •).pdf", "ìê¸ˆìš´ìš©ì—…ë¬´ì— ê´€í•œ ì„¸ì¹™(2024ë…„ 12ì›” ê°œì •).pdf",
            "íˆ¬ì ë° ìê¸ˆìš´ìš©ì—…ë¬´ ì‹¬ì‚¬ì— ê´€í•œ ì„¸ì¹™(2023ë…„ 6ì›” ê°œì •).pdf"
        ]
    },
    "6": {
        "name": "ì†Œì†¡,ê·œì •",
        "search_terms": ["ì†Œì†¡ì—…ë¬´ì²˜ë¦¬ê·œì •", "ì„ì§ì›ì†Œì†¡ì§€ì›ê·œì •", "ì œê·œì •ê´€ë¦¬ê·œì •"],
        "pdf_files": ["ì†Œì†¡ì—…ë¬´ì²˜ë¦¬ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf", "ì„ì§ì›ì†Œì†¡ì§€ì›ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf", "ì œê·œì •ê´€ë¦¬ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf"]
    },
    "7": {
        "name": "ìœ¤ë¦¬,ì¸ê¶Œ",
        "search_terms": ["ê³µì •ê±°ë˜ ììœ¨ì¤€ìˆ˜í”„ë¡œê·¸ë¨ ìš´ì˜ê·œì •", "ê¸°ì—…ì„±ì¥ì‘ë‹µì„¼í„° ìš´ì˜ê·œì •", "ì„ì›ì§ë¬´ì²­ë ´ê³„ì•½ìš´ì˜ê·œì •"],
        "pdf_files": [
            "ê³µì •ê±°ë˜ ììœ¨ì¤€ìˆ˜í”„ë¡œê·¸ë¨ ìš´ì˜ê·œì •(2022ë…„ 1ì›” ì œì •).pdf", "ê¸°ì—…ì„±ì¥ì‘ë‹µì„¼í„° ìš´ì˜ê·œì •(2023ë…„ 12ì›” ê°œì •).pdf",
            "ì„ì›ì§ë¬´ì²­ë ´ê³„ì•½ìš´ì˜ê·œì •(2021ë…„ 10ì›” ê°œì •).pdf"
        ]
    },
    "8": {
        "name": "ê°ì‚¬",
        "search_terms": ["ê°ì‚¬ê·œì •", "ë‚´ë¶€í†µì œê·œì •", "ë¶€ì •ì²­íƒ ë° ê¸ˆí’ˆë“± ìˆ˜ìˆ˜ì˜ ì‹ ê³ ì‚¬ë¬´ ì²˜ë¦¬ì§€ì¹¨", "ì´í•´ì¶©ëŒ ë°©ì§€ì œë„ ìš´ì˜ì§€ì¹¨"],
        "pdf_files": [
            "ê°ì‚¬ê·œì •(2024ë…„ 7ì›” ê°œì •).pdf", "ë‚´ë¶€í†µì œê·œì •(2025ë…„ 3ì›” ê°œì •).pdf", "ë¶€ì •ì²­íƒ ë° ê¸ˆí’ˆë“± ìˆ˜ìˆ˜ì˜ ì‹ ê³ ì‚¬ë¬´ ì²˜ë¦¬ì§€ì¹¨ 171207.pdf",
            "ì´í•´ì¶©ëŒ ë°©ì§€ì œë„ ìš´ì˜ì§€ì¹¨(2024ë…„ 10ì›” ê°œì •).pdf"
        ]
    },
    "9": {
        "name": "ì•ˆì „,ë³´ê±´",
        "search_terms": ["ì•ˆì „ë³´ê±´ê´€ë¦¬ê·œì •"],
        "pdf_files": ["ì•ˆì „ë³´ê±´ê´€ë¦¬ê·œì •(2025ë…„ 1ì›” ê°œì •).pdf"]
    },
    "10": {
        "name": "ë³´ì•ˆ",
        "search_terms": ["ë³´ì•ˆì—…ë¬´ê·œì¹™"],
        "pdf_files": ["ë³´ì•ˆì—…ë¬´ê·œì¹™(2024ë…„ 5ì›” ê°œì •).pdf"]
    },
    "11": {
        "name": "ì¼ë°˜í–‰ì •",
        "search_terms": ["ê¸°ì—…ë¯¼ì› ë³´í˜¸ì„œë¹„ìŠ¤ í—Œì¥ ìš´ì˜ê·œì •", "ë¯¼ì›ì²˜ë¦¬ê·œì •", "ì—¬ë¹„ê·œì •", "ì²­ì›ì‹¬ì˜íšŒ ìš´ì˜ê·œì •", "í–‰ì •ì—…ë¬´ì˜ íš¨ìœ¨ì  ìš´ì˜ì— ê´€í•œ ê·œì •"],
        "pdf_files": [
            "ê¸°ì—…ë¯¼ì› ë³´í˜¸ì„œë¹„ìŠ¤ í—Œì¥ ìš´ì˜ê·œì •(210219).pdf", "ë¯¼ì›ì²˜ë¦¬ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf", "ì—¬ë¹„ê·œì •(2025ë…„ 2ì›” ê°œì •).pdf",
            "ì²­ì›ì‹¬ì˜íšŒ ìš´ì˜ê·œì •(2023ë…„ 6ì›” ì œì •).pdf", "í–‰ì •ì—…ë¬´ì˜ íš¨ìœ¨ì  ìš´ì˜ì— ê´€í•œ ê·œì •(2025ë…„ 3ì›” ê°œì •).pdf"
        ]
    },
    "12": {
        "name": "ê²€ì‚¬",
        "search_terms": ["ê¸°ê³„ì‹ì£¼ì°¨ì¥ì—…ë¬´ê·œì •", "ë‚´ì••ìš©ê¸°ê²€ì‚¬ì—…ë¬´ê·œì •", "ìë™ì°¨ê²€ì‚¬ì—…ë¬´ë“±ì—ê´€í•œê·œì •", "íŠ¹ìˆ˜ê²€ì‚¬ì—…ë¬´ì„¸ì¹™"],
        "pdf_files": [
            "ê¸°ê³„ì‹ì£¼ì°¨ì¥ì—…ë¬´ê·œì •(2024ë…„ 12ì›” ê°œì •).pdf", "ë‚´ì••ìš©ê¸°ê²€ì‚¬ì—…ë¬´ê·œì •(2024ë…„ 3ì›” ê°œì •).pdf",
            "ìë™ì°¨ê²€ì‚¬ì—…ë¬´ë“±ì—ê´€í•œê·œì •(ì „ë¬¸ 2019ë…„ 7ì›” 1ì¼).pdf", "íŠ¹ìˆ˜ê²€ì‚¬ì—…ë¬´ì„¸ì¹™(2024ë…„ 6ì›” ê°œì •).pdf"
        ]
    },
    "13": {
        "name": "ìê²©,êµìœ¡",
        "search_terms": ["êµ­ê°€ìê²©ì‹œí—˜ê´€ë¦¬ì„¸ì¹™", "ë¬´ì¸ë¹„í–‰ì¥ì¹˜ ì „ë¬¸êµìœ¡ê¸°ê´€ ì§€ì • ë° ê´€ë¦¬ì—…ë¬´ ìš´ì˜ì„¸ì¹™", "ë¬´ì¸ë¹„í–‰ì¥ì¹˜ ì¡°ì¢…ì ì¦ëª… ìš´ì˜ì„¸ì¹™", "ë¯¼ê°„ìê²©ê´€ë¦¬ìš´ì˜ì„¸ì¹™", "ìš´ì „ì ì„±ì •ë°€ê²€ì‚¬ ìš´ì˜ì„¸ì¹™", "ì´ˆê²½ëŸ‰ë¹„í–‰ì¥ì¹˜ ì¡°ì¢…ì ì¦ëª… ìš´ì˜ì„¸ì¹™"],
        "pdf_files": [
            "êµ­ê°€ìê²©ì‹œí—˜ê´€ë¦¬ì„¸ì¹™(2024ë…„ 12ì›” ê°œì •).pdf", "ë¬´ì¸ë¹„í–‰ì¥ì¹˜ ì „ë¬¸êµìœ¡ê¸°ê´€ ì§€ì • ë° ê´€ë¦¬ì—…ë¬´ ìš´ì˜ì„¸ì¹™(2022ë…„ 11ì›” ì œì •).pdf",
            "ë¬´ì¸ë¹„í–‰ì¥ì¹˜ ì¡°ì¢…ì ì¦ëª… ìš´ì˜ì„¸ì¹™(2025ë…„ 4ì›” ê°œì •).pdf", "ë¯¼ê°„ìê²©ê´€ë¦¬ìš´ì˜ì„¸ì¹™(191218).pdf",
            "ìš´ì „ì ì„±ì •ë°€ê²€ì‚¬ ìš´ì˜ì„¸ì¹™(191224).pdf", "ì´ˆê²½ëŸ‰ë¹„í–‰ì¥ì¹˜ ì¡°ì¢…ì ì¦ëª… ìš´ì˜ì„¸ì¹™(2024ë…„ 7ì›” ê°œì •).pdf"
        ]
    },
    "14": {
        "name": "ì² ë„",
        "search_terms": ["ì •ë°€ì§„ë‹¨ ì„±ëŠ¥í‰ê°€_ê²°ê³¼ë³´ê³ ì„œ í‰ê°€ ê·œì •"],
        "pdf_files": ["ì •ë°€ì§„ë‹¨ ì„±ëŠ¥í‰ê°€_ê²°ê³¼ë³´ê³ ì„œ í‰ê°€ ê·œì •(2023ë…„ 12ì›” ì œì •).pdf"]
    },
    "15": {
        "name": "ì—°êµ¬",
        "search_terms": ["ì—°êµ¬ê´€ë¦¬ê·œì •", "ìš©ì—­ì—…ë¬´ìˆ˜íƒì—ê´€í•œì„¸ì¹™", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì—…ë¬´ì„¸ì¹™"],
        "pdf_files": ["ì—°êµ¬ê´€ë¦¬ê·œì •(2025ë…„ 3ì›” ê°œì •).pdf", "ìš©ì—­ì—…ë¬´ìˆ˜íƒì—ê´€í•œì„¸ì¹™(2024ë…„ 9ì›” ê°œì •).pdf", "ìë™ì°¨ì•ˆì „ì—°êµ¬ì› ì—…ë¬´ì„¸ì¹™(2022ë…„ 6ì›” ê°œì •).pdf"]
    },
    "16": {
        "name": "ê¸°íƒ€ì‚¬ì—…",
        "search_terms": ["ì‚¬ë‚´ë²¤ì²˜ ìš´ì˜ê·œì •", "ì´ˆê²½ëŸ‰ë¹„í–‰ì¥ì¹˜ ì‹ ê³ ì—…ë¬´ ìš´ì˜ì„¸ì¹™", "íŠœë‹ë¶€í’ˆ ì•ˆì „í™•ì¸ ì—…ë¬´ ë“±ì— ê´€í•œ ê·œì •"],
        "pdf_files": [
            "ì‚¬ë‚´ë²¤ì²˜ ìš´ì˜ê·œì •(2022ë…„ 12ì›” ê°œì •).pdf", "ì´ˆê²½ëŸ‰ë¹„í–‰ì¥ì¹˜ ì‹ ê³ ì—…ë¬´ ìš´ì˜ì„¸ì¹™(2025ë…„ 4ì›” ê°œì •).pdf",
            "íŠœë‹ë¶€í’ˆ ì•ˆì „í™•ì¸ ì—…ë¬´ ë“±ì— ê´€í•œ ê·œì •(2025ë…„ 2ì›” ê°œì •).pdf"
        ]
    }
}
ALL_FILES = list(set([term for category in CATEGORIES.values() for term in category['search_terms']]))
JSON_FILE_PATH = 'í†µí•©_ìˆ˜ì •ì™„ë£Œ.json'


@st.cache_resource
def load_sbert_model():
    return SentenceTransformer('jhgan/ko-sbert-nli')

@st.cache_data
def load_and_embed_data(path, _model):
    try:
        with open(path, 'r', encoding='utf-8') as f: data = json.load(f)
    except: return None
    processed_docs = []
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§) ...
    for item in data:
        file_name = item.get('fileName', '')
        regulation_name = os.path.splitext(file_name)[0]
        regulation_name = re.sub(r'\(.*\)', '', regulation_name).strip()
        full_page_text = []
        for page in item.get('pages', []):
            for content_block in page.get('content', []):
                if content_block.get('type') == 'paragraph': full_page_text.append(content_block.get('data', ''))
                elif content_block.get('type') == 'table':
                    table_data = content_block.get('data', []); table_text = ' '.join([' '.join(map(str, row)) for row in table_data]); full_page_text.append(f"[í‘œ ë‚´ìš©] {table_text}")
        final_text = "\n".join(full_page_text)
        if final_text: processed_docs.append({"ê·œì •ëª…": regulation_name, "ë‚´ìš©": final_text, "id": len(processed_docs)})
    if not processed_docs: return None
    contents = [doc['ë‚´ìš©'] for doc in processed_docs]
    embeddings = _model.encode(contents, convert_to_tensor=True, show_progress_bar=True)
    return {"docs": processed_docs, "embeddings": embeddings.cpu().numpy()}

# --- 2. ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì—”ì§„ (ì´ì „ê³¼ ë™ì¼) ---
def hybrid_search(query, data, model, selected_files, top_k=3):
    query_keywords = set(query.split())
    docs_to_search = [doc for doc in data['docs'] if any(doc['ê·œì •ëª…'].startswith(f) for f in selected_files)]
    keyword_results = []
    for doc in docs_to_search:
        doc_words = set(doc['ë‚´ìš©'].split()); matched_keywords = query_keywords.intersection(doc_words)
        if matched_keywords: keyword_results.append({'doc': doc, 'score': len(matched_keywords)})
    keyword_results = sorted(keyword_results, key=lambda x: x['score'], reverse=True)
    query_embedding = model.encode(query, convert_to_tensor=True).cpu().numpy()
    indices_to_search = [i for i, doc in enumerate(data['docs']) if any(doc['ê·œì •ëª…'].startswith(f) for f in selected_files)]
    if not indices_to_search: return []
    filtered_docs_for_semantic = [data['docs'][i] for i in indices_to_search]
    filtered_embeddings = data['embeddings'][indices_to_search]
    semantic_similarities = cosine_similarity(query_embedding.reshape(1, -1), filtered_embeddings)[0]
    fused_scores = {}; k = 60
    semantic_top_indices = np.argsort(semantic_similarities)[::-1]
    for rank, idx in enumerate(semantic_top_indices):
        doc_id = filtered_docs_for_semantic[idx]['id']
        if doc_id not in fused_scores: fused_scores[doc_id] = 0
        fused_scores[doc_id] += 1 / (k + rank + 1)
    for rank, res in enumerate(keyword_results):
        doc_id = res['doc']['id']
        if doc_id not in fused_scores: fused_scores[doc_id] = 0
        fused_scores[doc_id] += 1 / (k + rank + 1)
    if not fused_scores: return []
    sorted_doc_ids = sorted(fused_scores.keys(), key=lambda id: fused_scores[id], reverse=True)
    id_to_doc = {doc['id']: doc for doc in data['docs']}
    return [id_to_doc[doc_id] for doc_id in sorted_doc_ids[:top_k]]

def generate_ultimate_answer(query, context_docs, chat_history):
    try:
        api_key = st.secrets.get("GOOGLE_API_KEY") if hasattr(st, 'secrets') and st.secrets.get("GOOGLE_API_KEY") else st.session_state.get("api_key")
        if not api_key: return "ì˜¤ë¥˜: API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        context_str = "\n\n---\n\n".join([f"ë¬¸ì„œ ì œëª©: {doc['ê·œì •ëª…']}\në‚´ìš©: {doc['ë‚´ìš©']}" for doc in context_docs])
        history_str = "\n".join([f'{msg["role"]}: {msg["content"]}' for msg in chat_history])
        prompt = f"""ë‹¹ì‹ ì€ í•µì‹¬ë§Œ ë§í•˜ëŠ” ê³¼ë¬µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
### ì§€ì‹œì‚¬í•­:
1. 'ì´ì „ ëŒ€í™”'ì™€ 'ê´€ë ¨ ê·œì • ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ 'í˜„ì¬ ì§ˆë¬¸'ì˜ í•µì‹¬ì ì¸ ë‹µ(Fact)ì„ ì°¾ìœ¼ì„¸ìš”.
2. ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¨ í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
3. ì ˆëŒ€ ë¶€ì—° ì„¤ëª…ì„ í•˜ê±°ë‚˜, ë¨¼ì € ì œì•ˆí•˜ê±°ë‚˜, ì¸ì‚¬ë§ì„ í•˜ì§€ ë§ˆì„¸ìš”.
4. ì˜¤ì§ ì‚¬ì‹¤ì— ê¸°ë°˜í•œ í•œ ë¬¸ì¥ì˜ ë‹µë³€ë§Œ ì œê³µí•˜ê³  ì‚¬ìš©ìì˜ ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë‹¤ë¦¬ì„¸ìš”.
### ì´ì „ ëŒ€í™”:
{history_str}
### ê´€ë ¨ ê·œì • ë‚´ìš©:
{context_str}
### í˜„ì¬ ì§ˆë¬¸:
{query}
### ì „ë¬¸ê°€ì˜ í•œ ë¬¸ì¥ ë‹µë³€:
"""
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- 3. UI êµ¬ì„± ë° ë©”ì¸ ë¡œì§ ---
st.title("ğŸ”— ì—…ë¬´ê·œì • AI")

with st.sidebar:
    st.title("ì„¤ì • ë° ë‹¤ìš´ë¡œë“œ")
    st.session_state.api_key = st.text_input("Google AI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    st.markdown("---")
    
    # âœ¨ ì¤‘ì²© ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë””ìì¸ ë³€ê²½
    with st.expander("ê·œì •ì§‘ ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", expanded=True):
        for cat_id, cat_info in CATEGORIES.items():
            if cat_info.get('pdf_files'): # PDF íŒŒì¼ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬ë§Œ í‘œì‹œ
                # ì¹´í…Œê³ ë¦¬ ì œëª©ì„ ì†Œì œëª©ìœ¼ë¡œ í‘œì‹œ
                st.subheader(f"ğŸ“‚ {cat_info['name']}")
                for filename in cat_info['pdf_files']:
                    file_path = os.path.join("pdf_files", filename)
                    try:
                        with open(file_path, "rb") as pdf_file:
                            PDFbyte = pdf_file.read()
                        st.download_button(
                            label=f"ğŸ“„ {filename.split('(')[0].split('.')[0]}",
                            data=PDFbyte,
                            file_name=filename,
                            mime='application/octet-stream',
                            use_container_width=True
                        )
                    except FileNotFoundError:
                        st.warning(f"'{filename}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.markdown("---") # ì¹´í…Œê³ ë¦¬ë³„ êµ¬ë¶„ì„ 


sbert_model = load_sbert_model()
data = load_and_embed_data(JSON_FILE_PATH, sbert_model)
if data is None: st.error("ê·œì •ì§‘ ë°ì´í„° íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); st.stop()

if "category" not in st.session_state: st.session_state.category = None
if "messages" not in st.session_state: st.session_state.messages = []

if st.session_state.category is None:
    st.info("í†µí•© ê²€ìƒ‰ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    if st.button("ğŸŒ ì „ì²´ ê·œì •ì—ì„œ ê²€ìƒ‰", use_container_width=True, type="primary"):
        st.session_state.category = "global"; st.rerun()
    st.markdown("---")
    sorted_categories = sorted(CATEGORIES.items(), key=lambda item: int(item[0]))
    for key, value in sorted_categories:
        if st.button(value["name"], key=key, use_container_width=True):
            st.session_state.category = key; st.rerun()
else:
    if st.session_state.category == "global":
        category_name, files_to_search = "ì „ì²´ ê·œì •", ALL_FILES
    else:
        info = CATEGORIES[st.session_state.category]
        category_name, files_to_search = info['name'], info['search_terms']
    
    st.header(f"ğŸ’¬ {category_name} ê²€ìƒ‰")
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]): st.markdown(msg["content"])

    if prompt := st.chat_input(f"'{category_name}'ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        with st.chat_message("assistant"):
            api_key_to_use = st.secrets.get("GOOGLE_API_KEY") if hasattr(st, 'secrets') and st.secrets.get("GOOGLE_API_KEY") else st.session_state.get("api_key")
            if not api_key_to_use:
                st.error("AI ë‹µë³€ì„ ìƒì„±í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì— Google AI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ê°€ì¥ ì •í™•í•œ ê·œì •ì„ ì°¾ê³  ë‹µë³€ì˜ í•µì‹¬ì„ ìš”ì•½í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    chat_history = st.session_state.messages[-7:-1]
                    context_docs = hybrid_search(prompt, data, sbert_model, files_to_search)
                    if not context_docs:
                        response = "ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    else:
                        response = generate_ultimate_answer(prompt, context_docs, chat_history)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

    if st.button("â†©ï¸ ì¹´í…Œê³ ë¦¬ ì„ íƒìœ¼ë¡œ"):
        st.session_state.category = None; st.session_state.messages = []; st.rerun()